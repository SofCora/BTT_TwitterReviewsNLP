{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Implement Your Machine Learning Project Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab assignment, you will implement the machine learning project plan you created in the written assignment. You will:\n",
    "\n",
    "1. Load your data set and save it to a Pandas DataFrame.\n",
    "2. Perform exploratory data analysis on your data to determine which feature engineering and data preparation techniques you will use.\n",
    "3. Prepare your data for your model and create features and a label.\n",
    "4. Fit your model to the training data and evaluate your model.\n",
    "5. Improve your model by performing model selection and/or feature selection techniques to find best model for your problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "Before you get started, import a few packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> In the code cell below, import additional packages that you have used in this course that you will need for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import tensorflow.keras as keras\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras import layers\n",
    "import gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load the Data Set\n",
    "\n",
    "\n",
    "I chose the book reviews dataset becasue I wanted to expand upon NLP and learn more about deep neural networks. The overall structure of my model is word embeddings that are mapped to tokens and fed into a recurrent neural network. The word embeddings are input using an embedding layer, it goes through 2 LSTM layers and then an output layer.\n",
    "I wanted to use word embeddings because they aren't sparse like other vectorizers and uitilzes the semantic context between each word through cosine similarity\n",
    "\n",
    "Because what I wanted to do was a bit out of the scope of what we covered in class, I used the following resources to create a shell for my model and figure out how to process word embeddings: \n",
    "<a href=\"https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\">word embeddings tutoiral</a>\n",
    "<a href=\"https://www.google.com/url?q=https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test&sa=D&source=docs&ust=1691811932916951&usg=AOvVaw04Jx8dffs2JwJipeaD3cKQ\">how to split data into a val set</a>\n",
    "<a href=\"https://keras.io/guides/working_with_rnns/#:~:text=model%20%3D%20keras.Sequential%28%29%20%23%20Add%20an%20Embedding%20layer,a%20Dense%20layer%20with%2010%20units.%20model.add%28layers.Dense%2810%29%29%20model.summary%28%29\">Keras RNN Documentation</a>\n",
    "<a href=\"https://danijar.com/tips-for-training-recurrent-neural-networks/\">Optimizing</a>\n",
    "<a href=\"https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/\">LSTM</a>\n",
    "<a href=\"https://www.bing.com/search?q=gru+layers+in+rnn&cvid=d4fca12d48024661902a22055ced9c00&aqs=edge.0.69i59j69i57j0l5j46j69i60.1461j0j9&FORM=ANAB01&PC=U531\">GRU</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "filename = os.path.join(os.getcwd(), \"data\", \"bookReviewsData.csv\")\n",
    "df = pd.read_csv(filename, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis\n",
    "\n",
    "The next step is to inspect and analyze your data set with your machine learning problem and project plan in mind. \n",
    "\n",
    "This step will help you determine data preparation and feature engineering techniques you will need to apply to your data to build a balanced modeling data set for your problem and model. These data preparation techniques may include:\n",
    "* addressing missingness, such as replacing missing values with means\n",
    "* renaming features and labels\n",
    "* finding and replacing outliers\n",
    "* performing winsorization if needed\n",
    "* performing one-hot encoding on categorical features\n",
    "* performing vectorization for an NLP problem\n",
    "* addressing class imbalance in your data sample to promote fair AI\n",
    "\n",
    "\n",
    "<h2>EDA</h2>\n",
    "Changed the column name so there's no whitespace. Checked for class imbalance and null values. Changed Review column format so its numbers instead of booleans for easier processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993\n",
      "980\n",
      "no class imbalance\n",
      "are there any null values? Review         False\n",
      "is_Positive    False\n",
      "dtype: bool\n",
      "                                              Review  is_Positive\n",
      "0  This was perhaps the best of Johannes Steinhof...         True\n",
      "1  This very fascinating book is a story written ...         True\n",
      "2  The four tales in this collection are beautifu...         True\n",
      "3  The book contained more profanity than I expec...        False\n",
      "4  We have now entered a second time of deep conc...         True\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "df.rename(columns = {'Positive Review':'is_Positive'}, inplace = True)\n",
    "#no class imbalance its like 980 positive 993 negative\n",
    "print(df['is_Positive'].value_counts()[False])\n",
    "print(df['is_Positive'].value_counts()[True])\n",
    "print('no class imbalance')\n",
    "print('are there any null values?', df.isna().any())\n",
    "df['is_Positive']= df['is_Positive'].astype(int)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>is_Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was perhaps the best of Johannes Steinhof...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This very fascinating book is a story written ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The four tales in this collection are beautifu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The book contained more profanity than I expec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We have now entered a second time of deep conc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  is_Positive\n",
       "0  This was perhaps the best of Johannes Steinhof...            1\n",
       "1  This very fascinating book is a story written ...            1\n",
       "2  The four tales in this collection are beautifu...            1\n",
       "3  The book contained more profanity than I expec...            0\n",
       "4  We have now entered a second time of deep conc...            1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Possible Bias in Dataset </h2>\n",
    "I noticed the number of positive reviews with male pronouns was double\n",
    "the amount of positive reviews with female pronouns, implying there's more positive reviews for male authors than female.\n",
    "I decided to use gendered pronouns as a stopwords list to clean the data with to prevent gender bias as a preventative measure.\n",
    "If I hadn't the model could fall into allocative bias, not recommending books by female authors as often than male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many reviews with male pronouns that are positive: 359\n",
      "how many reviews with female pronouns are positive: 168\n"
     ]
    }
   ],
   "source": [
    "female_pronouns = [ 'she', 'her']\n",
    "male_pronouns = ['he', 'him', 'his']\n",
    "\n",
    "# Combine the target words into a regular expression pattern\n",
    "m_pattern = r'\\b(?:' + '|'.join(male_pronouns) + r')\\b'\n",
    "f_pattern =  r'\\b(?:' + '|'.join(female_pronouns) + r')\\b'\n",
    "\n",
    "male_df = df[df['Review'].str.contains(m_pattern, case=False, na=False)]\n",
    "female_df = df[df['Review'].str.contains(f_pattern, case=False, na=False)]\n",
    "print('how many reviews with male pronouns that are positive:', (male_df['is_Positive'] == 1).sum())\n",
    "print('how many reviews with female pronouns are positive:',(female_df['is_Positive'] == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/codio/.local/lib/python3.6/site-packages (3.6.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.14.1)\n",
      "Requirement already satisfied: tqdm in /home/codio/.local/lib/python3.6/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/codio/.local/lib/python3.6/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: click in /home/codio/.local/lib/python3.6/site-packages (from nltk) (7.1.1)\n",
      "Requirement already satisfied: importlib-resources in /home/codio/.local/lib/python3.6/site-packages (from tqdm->nltk) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/codio/.local/lib/python3.6/site-packages (from importlib-resources->tqdm->nltk) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/codio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocess Data and Tokenize it</h2>\n",
    "While cutting stopwords usually improves model performance, I noticed that the more stopwords I excluded, the worse the model performed, this could be because I'm not working with very long documents (each review is a paragraph each) or because the word vectors/ RNN are able to better use the context of stopwords to make predictions.\n",
    "So i only took out the gendered pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "review_lines = list()\n",
    "liness = df['Review'].values.tolist()\n",
    "\n",
    "lines = df['Review']\n",
    "lines = lines.apply(lambda row: gensim.utils.simple_preprocess(row))\n",
    "\n",
    "for line in liness:\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    review_lines.append(words)\n",
    "print(review_lines)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many examples :  1973\n"
     ]
    }
   ],
   "source": [
    "review_lines = []  # Initialize the list to store cleaned lines\n",
    "\n",
    "# Preprocess the lines using simple_preprocess and remove stop words\n",
    "lines = df['Review']\n",
    "stop_words = ['he', 'his', 'him', 'she', 'her', 'hers']\n",
    "\n",
    "for line in lines:\n",
    "    cleaned_line = [word for word in gensim.utils.simple_preprocess(line) if word not in stop_words]\n",
    "    review_lines.append(cleaned_line)\n",
    "\n",
    "print('how many examples : ',len(review_lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating the Word Embeddings Using Word2Vec</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size 20837\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "model = gensim.models.Word2Vec(sentences = review_lines, vector_size = EMBEDDING_DIM, window = 5, workers = 4, min_count = 1)\n",
    "words = list(model.wv.key_to_index)\n",
    "print('vocabulary size', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('premise', 0.995363175868988),\n",
       " ('unfortunately', 0.9951717257499695),\n",
       " ('version', 0.9941315650939941),\n",
       " ('volume', 0.9941017031669617),\n",
       " ('place', 0.9940797090530396),\n",
       " ('reference', 0.9940342307090759),\n",
       " ('text', 0.9939666390419006),\n",
       " ('wonderful', 0.9939011335372925),\n",
       " ('plot', 0.9938108921051025),\n",
       " ('sad', 0.9933741092681885)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('trilogy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Professor Osuno asked me to create a wordcloud for the most frequent words \n",
    "When I tried the following code I got this error:: ValueError: Only supported for TrueType fonts\n",
    "\n",
    "top25 = model.wv.index_to_key[:25] \n",
    "from wordcloud import WordCloud\n",
    "text = ' '.join(list(top25))\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=800, background_color='white', min_font_size=10).generate(text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "I tried specifying the font_path hyperparameter to use a font already installed in this enviornment but I was gettign the same errors. Additinoally I tried updating the wordcloud package and got this error:: Defaulting to user installation because normal site-packages is not writeable Requirement already \n",
    "\n",
    "I believe the Codio Jupyter Notebook enviornment is contributing to these errors and the word cloud would otherwise work on another IDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating a Dictionary With Word:Vector Pairs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the wordembeddings as a dict of words to vectors\n",
    "filename = 'book_review_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary = False)\n",
    "embeddings_dictionary = {}\n",
    "f = open(os.path.join('', 'book_review_word2vec.txt'), encoding = 'utf-8')\n",
    "#so one line of f is the word corresponding with a long integer vector\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_dictionary[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't just put the word vectors straight into a neural network \n",
    "so they need to be tokenized and put into an embedding layer rather than as input for like a normal dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert word embedding into a tokenized vector. maps to the index of a single vector in the embeddings layer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(review_lines)\n",
    "sequences = tokenizer_obj.texts_to_sequences(review_lines)\n",
    "\n",
    "#this is to standardize the length of the vectors which i guess are now sequences \n",
    "#using sequence paddings\n",
    "\n",
    "word_index = tokenizer_obj.word_index\n",
    "#word index is like a dictionary of a word and a number 1-20843\n",
    "review_pad = pad_sequences(sequences, maxlen = 100)\n",
    "#so in each index of review pad is a bunch of numbers based on the embeddings\n",
    "# theres always 100 numbers at each index because thats what maxlen is\n",
    "#if vector is <100 then add 0s\n",
    "is_Positive = df['is_Positive'].values\n",
    "#max_len is a tunable hyperparameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so now were mapping the embeddings from word2vec to the tokenizer vocab to make a matrix\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "#embedding_matrix is the final var that you input into the embedding layer in final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implement Your Project Plan\n",
    "\n",
    "<b>Task:</b> Use the rest of this notebook to carry out your project plan. You will:\n",
    "\n",
    "1. Prepare your data for your model and create features and a label.\n",
    "2. Fit your model to the training data and evaluate your model.\n",
    "3. Improve your model by performing model selection and/or feature selection techniques to find best model for your problem.\n",
    "\n",
    "\n",
    "Add code cells below and populate the notebook with commentary, code, analyses, results, and figures as you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Im going to create the neural net starting with an embedding layer that will take in our sequences/embeddings/dictionary/matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU \n",
    "#those are diff types of rnn layers\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "from tensorflow.keras.metrics import AUC, Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>What are the RNN Layers?</h2>\n",
    "<ul>\n",
    "    <li>Embedding - maps input information from a high-dimensional to a lower-dimensional space</li>\n",
    "<li>LSTM - Solves the vanishing gradient problem (gradient becomes too small to update weights) cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell. theres a cell, Forget gate, input and output gates</li>\n",
    "<li>GRU - Like LSTM but fewer parameters and lacks an output gate. Reset and update gates no long range memory cell</li>\n",
    "</ul>\n",
    "\n",
    "Now I'm going to do a train/val/split where validation set is 20% of the total data\n",
    "basically u take the review_pad sequence, randomize all the indices and the data up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "is_Positive = is_Positive[indices]\n",
    "\n",
    "\n",
    "#so idk how to get 3 splits with train_test_split so i did two test train splits on top of each other\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(review_pad,is_Positive,test_size=0.2,train_size=0.8)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size = 0.25,train_size =0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Performing A Grid Search For Best Params </h2>\n",
    "I'm trying to optimize the number of units for each layer as well as the optimizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "def build_clf(unit1, unit2, losses):\n",
    "  # creating the layers of the RNN\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(num_words, EMBEDDING_DIM, \n",
    "                           embeddings_initializer = Constant(embedding_matrix),\n",
    "                           input_length = 100,\n",
    "                           trainable = False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(units=unit1, dropout = .20, recurrent_dropout = .20, return_sequences = True))\n",
    "    model.add(LSTM(units=32, dropout = .20, recurrent_dropout = .20, return_sequences = True))\n",
    "    model.add(LSTM(units=unit2, dropout = .20, recurrent_dropout = .20))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(loss=losses, optimizer='adam', metrics=[AUC()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = KerasClassifier(build_fn=build_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning grid search\n",
      "25/25 [==============================] - 23s 229ms/step - loss: 0.6972 - auc_206: 0.4848\n",
      "25/25 [==============================] - 11s 198ms/step - loss: 0.6957 - auc_207: 0.5119\n",
      "25/25 [==============================] - 13s 200ms/step - loss: 0.6977 - auc_208: 0.5134\n",
      "25/25 [==============================] - 10s 187ms/step - loss: 0.6938 - auc_209: 0.5162\n",
      "25/25 [==============================] - 10s 169ms/step - loss: 0.6968 - auc_210: 0.4898\n",
      "25/25 [==============================] - 11s 191ms/step - loss: 0.6971 - auc_211: 0.5117\n",
      "25/25 [==============================] - 12s 212ms/step - loss: 0.6974 - auc_212: 0.5197\n",
      "25/25 [==============================] - 11s 229ms/step - loss: 0.6994 - auc_213: 0.4883\n",
      "25/25 [==============================] - 11s 197ms/step - loss: 0.6979 - auc_214: 0.5014\n",
      "25/25 [==============================] - 10s 175ms/step - loss: 0.6960 - auc_215: 0.5059\n",
      "25/25 [==============================] - 10s 176ms/step - loss: 0.7024 - auc_216: 0.4916\n",
      "25/25 [==============================] - 10s 175ms/step - loss: 0.7044 - auc_217: 0.4838\n",
      "25/25 [==============================] - 12s 209ms/step - loss: 0.7010 - auc_218: 0.4928\n",
      "25/25 [==============================] - 11s 212ms/step - loss: 0.7015 - auc_219: 0.4749\n",
      "25/25 [==============================] - 12s 274ms/step - loss: 0.7067 - auc_220: 0.4853\n",
      "25/25 [==============================] - 11s 216ms/step - loss: 0.7024 - auc_221: 0.5238\n",
      "25/25 [==============================] - 13s 242ms/step - loss: 0.6998 - auc_222: 0.4823\n",
      "25/25 [==============================] - 14s 268ms/step - loss: 0.6935 - auc_223: 0.5116\n",
      "25/25 [==============================] - 13s 242ms/step - loss: 0.7049 - auc_224: 0.4810\n",
      "25/25 [==============================] - 13s 280ms/step - loss: 0.7025 - auc_225: 0.4980\n",
      "25/25 [==============================] - 14s 288ms/step - loss: 0.7002 - auc_226: 0.5352\n",
      "25/25 [==============================] - 12s 251ms/step - loss: 0.7033 - auc_227: 0.5031\n",
      "25/25 [==============================] - 12s 244ms/step - loss: 0.6993 - auc_228: 0.5463\n",
      "25/25 [==============================] - 12s 250ms/step - loss: 0.7079 - auc_229: 0.4815\n",
      "25/25 [==============================] - 13s 268ms/step - loss: 0.7052 - auc_230: 0.4885\n",
      "25/25 [==============================] - 13s 276ms/step - loss: 0.7065 - auc_231: 0.4968\n",
      "25/25 [==============================] - 14s 288ms/step - loss: 0.6964 - auc_232: 0.5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: in user code:\n",
      "\n",
      "    /usr/local/lib/python3.6/dist-packages/keras/engine/training.py:853 train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    /usr/local/lib/python3.6/dist-packages/keras/engine/training.py:842 step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n",
      "        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n",
      "        return self._call_for_each_replica(fn, args, kwargs)\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n",
      "        return fn(*args, **kwargs)\n",
      "    /usr/local/lib/python3.6/dist-packages/keras/engine/training.py:835 run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    /usr/local/lib/python3.6/dist-packages/keras/engine/training.py:789 train_step\n",
      "        y, y_pred, sample_weight, regularization_losses=self.losses)\n",
      "    /usr/local/lib/python3.6/dist-packages/keras/engine/compile_utils.py:184 __call__\n",
      "        self.build(y_pred)\n",
      "    /usr/local/lib/python3.6/dist-packages/keras/engine/compile_utils.py:133 build\n",
      "        self._losses = tf.nest.map_structure(self._get_loss_object, self._losses)\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:869 map_structure\n",
      "        structure[0], [func(*x) for x in entries],\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:869 <listcomp>\n",
      "        structure[0], [func(*x) for x in entries],\n",
      "    /usr/local/lib/python3.6/dist-packages/keras/engine/compile_utils.py:273 _get_loss_object\n",
      "        loss = losses_mod.get(loss)\n",
      "    /usr/local/lib/python3.6/dist-packages/keras/losses.py:2136 get\n",
      "        return deserialize(identifier)\n",
      "    /usr/local/lib/python3.6/dist-packages/keras/losses.py:2095 deserialize\n",
      "        printable_module_name='loss function')\n",
      "    /usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py:709 deserialize_keras_object\n",
      "        .format(printable_module_name, object_name))\n",
      "\n",
      "    ValueError: Unknown loss function: kullback_leibler. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
      "\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 14s 180ms/step - loss: 0.6955 - auc_260: 0.5014\n",
      "Done\n",
      "best parameters: {'losses': 'binary_crossentropy', 'unit1': 32, 'unit2': 32}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "params = {\n",
    "    \"unit1\": [16, 32, 64, 128],\n",
    "    \"unit2\": [32, 64, 128, 160],\n",
    "}\n",
    "scoring = make_scorer(roc_auc_score)\n",
    "\n",
    "# Perform grid search with AUC as the scoring metric\n",
    "print(\"beginning grid search\")\n",
    "grid = GridSearchCV(keras_model, params, scoring=scoring, cv=3)\n",
    "grid_search = grid.fit(X_train, y_train)\n",
    "print('Done')\n",
    "print(\"best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Creating the Final Model </h1>\n",
    "<h3>Why I chose the Layer Type </h3>\n",
    "The LSTM is more complex than GRU because it has more gates and a memory cell that can better contextualize words in a document in long range dependencies\n",
    "<h3>Why I Chose the AUC Metric</h3>\n",
    "Because this model can be used for a recommender system, its worse to have a false positive than a false negative because you dont want to recommend a book thats disliked by readers. Because of that I ruled out recall. AUC is the amount of true positives over false positives, AUC also takes into account the probability that each prediction will be right so I chose AUC as my metric seems to be better for classifying text data than precision is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_267\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_267 (Embedding)    (None, 100, 100)          2083800   \n",
      "_________________________________________________________________\n",
      "lstm_533 (LSTM)              (None, 100, 32)           17024     \n",
      "_________________________________________________________________\n",
      "lstm_534 (LSTM)              (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense_267 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,183,385\n",
      "Trainable params: 99,585\n",
      "Non-trainable params: 2,083,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words, EMBEDDING_DIM, \n",
    "                           embeddings_initializer = Constant(embedding_matrix),\n",
    "                           input_length = 100,\n",
    "                           trainable = False)\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(units=32, dropout = .20, recurrent_dropout = .20, return_sequences = True))\n",
    "model.add(LSTM(units=128, dropout = .20, recurrent_dropout = .20))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[AUC()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theres a lot less trainable params than total params because I created  the word embeddings dictionary\n",
    "myself so in theory this will make it all train faster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training process\n",
      "Epoch 1/50\n",
      "10/10 - 20s - loss: 0.6937 - auc_267: 0.5118 - val_loss: 0.6903 - val_auc_267: 0.5553\n",
      "Epoch 2/50\n",
      "10/10 - 3s - loss: 0.6935 - auc_267: 0.5293 - val_loss: 0.6954 - val_auc_267: 0.5688\n",
      "Epoch 3/50\n",
      "10/10 - 3s - loss: 0.6918 - auc_267: 0.5344 - val_loss: 0.6864 - val_auc_267: 0.5653\n",
      "Epoch 4/50\n",
      "10/10 - 3s - loss: 0.6873 - auc_267: 0.5662 - val_loss: 0.6853 - val_auc_267: 0.5745\n",
      "Epoch 5/50\n",
      "10/10 - 3s - loss: 0.6868 - auc_267: 0.5660 - val_loss: 0.6830 - val_auc_267: 0.5795\n",
      "Epoch 6/50\n",
      "10/10 - 3s - loss: 0.6824 - auc_267: 0.5850 - val_loss: 0.6779 - val_auc_267: 0.5948\n",
      "Epoch 7/50\n",
      "10/10 - 3s - loss: 0.6806 - auc_267: 0.5907 - val_loss: 0.6816 - val_auc_267: 0.6023\n",
      "Epoch 8/50\n",
      "10/10 - 3s - loss: 0.6782 - auc_267: 0.5960 - val_loss: 0.6616 - val_auc_267: 0.6445\n",
      "Epoch 9/50\n",
      "10/10 - 4s - loss: 0.6705 - auc_267: 0.6221 - val_loss: 0.6604 - val_auc_267: 0.6512\n",
      "Epoch 10/50\n",
      "10/10 - 5s - loss: 0.6812 - auc_267: 0.5850 - val_loss: 0.6771 - val_auc_267: 0.6253\n",
      "Epoch 11/50\n",
      "10/10 - 4s - loss: 0.6808 - auc_267: 0.5919 - val_loss: 0.6698 - val_auc_267: 0.6459\n",
      "Epoch 12/50\n",
      "10/10 - 3s - loss: 0.6709 - auc_267: 0.6171 - val_loss: 0.6579 - val_auc_267: 0.6610\n",
      "Epoch 13/50\n",
      "10/10 - 3s - loss: 0.6671 - auc_267: 0.6303 - val_loss: 0.6447 - val_auc_267: 0.6795\n",
      "Epoch 14/50\n",
      "10/10 - 4s - loss: 0.6623 - auc_267: 0.6438 - val_loss: 0.6571 - val_auc_267: 0.6718\n",
      "Epoch 15/50\n",
      "10/10 - 4s - loss: 0.6797 - auc_267: 0.6037 - val_loss: 0.6622 - val_auc_267: 0.6626\n",
      "Epoch 16/50\n",
      "10/10 - 4s - loss: 0.6802 - auc_267: 0.5955 - val_loss: 0.6562 - val_auc_267: 0.6667\n",
      "Epoch 17/50\n",
      "10/10 - 3s - loss: 0.6746 - auc_267: 0.6428 - val_loss: 0.6693 - val_auc_267: 0.6408\n",
      "Epoch 18/50\n",
      "10/10 - 3s - loss: 0.6742 - auc_267: 0.6264 - val_loss: 0.6546 - val_auc_267: 0.6715\n",
      "Epoch 19/50\n",
      "10/10 - 4s - loss: 0.6659 - auc_267: 0.6388 - val_loss: 0.6642 - val_auc_267: 0.6734\n",
      "Epoch 20/50\n",
      "10/10 - 4s - loss: 0.6657 - auc_267: 0.6387 - val_loss: 0.6556 - val_auc_267: 0.6638\n",
      "Epoch 21/50\n",
      "10/10 - 3s - loss: 0.6635 - auc_267: 0.6462 - val_loss: 0.6599 - val_auc_267: 0.6537\n",
      "Epoch 22/50\n",
      "10/10 - 3s - loss: 0.6615 - auc_267: 0.6379 - val_loss: 0.6622 - val_auc_267: 0.6832\n",
      "Epoch 23/50\n",
      "10/10 - 3s - loss: 0.6547 - auc_267: 0.6576 - val_loss: 0.6736 - val_auc_267: 0.6926\n",
      "Epoch 24/50\n",
      "10/10 - 3s - loss: 0.6634 - auc_267: 0.6416 - val_loss: 0.6529 - val_auc_267: 0.6966\n",
      "Epoch 25/50\n",
      "10/10 - 3s - loss: 0.6597 - auc_267: 0.6570 - val_loss: 0.6600 - val_auc_267: 0.6784\n",
      "Epoch 26/50\n",
      "10/10 - 3s - loss: 0.6638 - auc_267: 0.6352 - val_loss: 0.6472 - val_auc_267: 0.6949\n",
      "Epoch 27/50\n",
      "10/10 - 3s - loss: 0.6415 - auc_267: 0.6796 - val_loss: 0.6348 - val_auc_267: 0.6905\n",
      "Epoch 28/50\n",
      "10/10 - 3s - loss: 0.6526 - auc_267: 0.6650 - val_loss: 0.6565 - val_auc_267: 0.6978\n",
      "Epoch 29/50\n",
      "10/10 - 3s - loss: 0.6599 - auc_267: 0.6541 - val_loss: 0.6283 - val_auc_267: 0.7030\n",
      "Epoch 30/50\n",
      "10/10 - 3s - loss: 0.6405 - auc_267: 0.6851 - val_loss: 0.6158 - val_auc_267: 0.7204\n",
      "Epoch 31/50\n",
      "10/10 - 4s - loss: 0.6368 - auc_267: 0.6882 - val_loss: 0.6338 - val_auc_267: 0.7222\n",
      "Epoch 32/50\n",
      "10/10 - 4s - loss: 0.6490 - auc_267: 0.6693 - val_loss: 0.6331 - val_auc_267: 0.7134\n",
      "Epoch 33/50\n",
      "10/10 - 5s - loss: 0.6487 - auc_267: 0.6689 - val_loss: 0.6207 - val_auc_267: 0.7187\n",
      "Epoch 34/50\n",
      "10/10 - 5s - loss: 0.6399 - auc_267: 0.6943 - val_loss: 0.6306 - val_auc_267: 0.7135\n",
      "Epoch 35/50\n",
      "10/10 - 8s - loss: 0.6359 - auc_267: 0.6906 - val_loss: 0.6266 - val_auc_267: 0.7145\n",
      "Epoch 36/50\n",
      "10/10 - 5s - loss: 0.6403 - auc_267: 0.6811 - val_loss: 0.6570 - val_auc_267: 0.7162\n",
      "Epoch 37/50\n",
      "10/10 - 3s - loss: 0.6387 - auc_267: 0.6854 - val_loss: 0.6221 - val_auc_267: 0.7120\n",
      "Epoch 38/50\n",
      "10/10 - 3s - loss: 0.6396 - auc_267: 0.6865 - val_loss: 0.6310 - val_auc_267: 0.7248\n",
      "Epoch 39/50\n",
      "10/10 - 3s - loss: 0.6302 - auc_267: 0.6974 - val_loss: 0.6120 - val_auc_267: 0.7244\n",
      "Epoch 40/50\n",
      "10/10 - 3s - loss: 0.6235 - auc_267: 0.7078 - val_loss: 0.6226 - val_auc_267: 0.7361\n",
      "Epoch 41/50\n",
      "10/10 - 3s - loss: 0.6224 - auc_267: 0.7124 - val_loss: 0.6121 - val_auc_267: 0.7381\n",
      "Epoch 42/50\n",
      "10/10 - 3s - loss: 0.6327 - auc_267: 0.6939 - val_loss: 0.6130 - val_auc_267: 0.7348\n",
      "Epoch 43/50\n",
      "10/10 - 3s - loss: 0.6113 - auc_267: 0.7258 - val_loss: 0.6160 - val_auc_267: 0.7314\n",
      "Epoch 44/50\n",
      "10/10 - 3s - loss: 0.6419 - auc_267: 0.6803 - val_loss: 0.6320 - val_auc_267: 0.7272\n",
      "Epoch 45/50\n",
      "10/10 - 3s - loss: 0.6274 - auc_267: 0.6999 - val_loss: 0.6273 - val_auc_267: 0.7153\n",
      "Epoch 46/50\n",
      "10/10 - 3s - loss: 0.6212 - auc_267: 0.7135 - val_loss: 0.5992 - val_auc_267: 0.7399\n",
      "Epoch 47/50\n",
      "10/10 - 3s - loss: 0.6133 - auc_267: 0.7250 - val_loss: 0.6041 - val_auc_267: 0.7426\n",
      "Epoch 48/50\n",
      "10/10 - 3s - loss: 0.6175 - auc_267: 0.7160 - val_loss: 0.6117 - val_auc_267: 0.7416\n",
      "Epoch 49/50\n",
      "10/10 - 3s - loss: 0.6060 - auc_267: 0.7354 - val_loss: 0.5968 - val_auc_267: 0.7551\n",
      "Epoch 50/50\n",
      "10/10 - 5s - loss: 0.6044 - auc_267: 0.7302 - val_loss: 0.6062 - val_auc_267: 0.7273\n",
      "training process done!!\n"
     ]
    }
   ],
   "source": [
    "print('begin training process')\n",
    "model.fit(X_train, y_train, batch_size = 128, epochs = 49, validation_data=(X_val, y_val), \n",
    "         verbose = 2)\n",
    "print('training process done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10/10 [==============================] - 4s 350ms/step - loss: 0.6023 - auc_267: 0.7377 - val_loss: 0.6001 - val_auc_267: 0.7385\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 4s 421ms/step - loss: 0.6126 - auc_267: 0.7260 - val_loss: 0.6192 - val_auc_267: 0.7138\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 3s 345ms/step - loss: 0.6232 - auc_267: 0.7106 - val_loss: 0.5946 - val_auc_267: 0.7547\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 4s 386ms/step - loss: 0.6253 - auc_267: 0.7049 - val_loss: 0.6027 - val_auc_267: 0.7649\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.6174 - auc_267: 0.7240 - val_loss: 0.6271 - val_auc_267: 0.7269\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.5956 - auc_267: 0.7453 - val_loss: 0.6342 - val_auc_267: 0.7342\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 0.6054 - auc_267: 0.7326 - val_loss: 0.6138 - val_auc_267: 0.7369\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 4s 393ms/step - loss: 0.5908 - auc_267: 0.7552 - val_loss: 0.6102 - val_auc_267: 0.7316\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 4s 368ms/step - loss: 0.5992 - auc_267: 0.7420 - val_loss: 0.6183 - val_auc_267: 0.7481\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 4s 387ms/step - loss: 0.6047 - auc_267: 0.7342 - val_loss: 0.5910 - val_auc_267: 0.7599\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 5s 462ms/step - loss: 0.5890 - auc_267: 0.7554 - val_loss: 0.6036 - val_auc_267: 0.7374\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 4s 394ms/step - loss: 0.5881 - auc_267: 0.7528 - val_loss: 0.6087 - val_auc_267: 0.7371\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 0.5966 - auc_267: 0.7421 - val_loss: 0.5889 - val_auc_267: 0.7642\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 0.5883 - auc_267: 0.7517 - val_loss: 0.6135 - val_auc_267: 0.7309\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 5s 471ms/step - loss: 0.6103 - auc_267: 0.7293 - val_loss: 0.6007 - val_auc_267: 0.7543\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 4s 406ms/step - loss: 0.5785 - auc_267: 0.7637 - val_loss: 0.5832 - val_auc_267: 0.7688\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.5692 - auc_267: 0.7701 - val_loss: 0.6147 - val_auc_267: 0.7251\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 4s 394ms/step - loss: 0.5661 - auc_267: 0.7793 - val_loss: 0.6174 - val_auc_267: 0.7429\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 4s 411ms/step - loss: 0.5699 - auc_267: 0.7747 - val_loss: 0.6040 - val_auc_267: 0.7424\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 4s 397ms/step - loss: 0.5486 - auc_267: 0.7936 - val_loss: 0.6105 - val_auc_267: 0.7463\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 4s 417ms/step - loss: 0.5588 - auc_267: 0.7853 - val_loss: 0.6159 - val_auc_267: 0.7422\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 4s 358ms/step - loss: 0.5673 - auc_267: 0.7745 - val_loss: 0.6106 - val_auc_267: 0.7372\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 4s 402ms/step - loss: 0.5556 - auc_267: 0.7880 - val_loss: 0.6122 - val_auc_267: 0.7317\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 4s 391ms/step - loss: 0.5309 - auc_267: 0.8114 - val_loss: 0.6341 - val_auc_267: 0.7221\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 4s 356ms/step - loss: 0.5487 - auc_267: 0.7935 - val_loss: 0.6120 - val_auc_267: 0.7344\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 4s 401ms/step - loss: 0.5396 - auc_267: 0.8049 - val_loss: 0.6093 - val_auc_267: 0.7430\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 5s 516ms/step - loss: 0.5414 - auc_267: 0.7988 - val_loss: 0.6471 - val_auc_267: 0.7186\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 4s 450ms/step - loss: 0.5612 - auc_267: 0.7844 - val_loss: 0.6304 - val_auc_267: 0.7164\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 5s 516ms/step - loss: 0.5701 - auc_267: 0.7736 - val_loss: 0.6072 - val_auc_267: 0.7324\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 4s 450ms/step - loss: 0.5238 - auc_267: 0.8159 - val_loss: 0.6415 - val_auc_267: 0.7195\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 4s 357ms/step - loss: 0.5379 - auc_267: 0.8018 - val_loss: 0.6155 - val_auc_267: 0.7359\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 3s 345ms/step - loss: 0.5233 - auc_267: 0.8214 - val_loss: 0.6347 - val_auc_267: 0.7203\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 3s 345ms/step - loss: 0.5212 - auc_267: 0.8219 - val_loss: 0.6282 - val_auc_267: 0.7309\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 3s 341ms/step - loss: 0.5292 - auc_267: 0.8111 - val_loss: 0.6546 - val_auc_267: 0.7146\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 4s 353ms/step - loss: 0.5219 - auc_267: 0.8167 - val_loss: 0.6578 - val_auc_267: 0.7141\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 4s 393ms/step - loss: 0.5264 - auc_267: 0.8164 - val_loss: 0.6266 - val_auc_267: 0.7267\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 4s 362ms/step - loss: 0.5149 - auc_267: 0.8250 - val_loss: 0.6089 - val_auc_267: 0.7468\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 4s 380ms/step - loss: 0.4926 - auc_267: 0.8416 - val_loss: 0.6307 - val_auc_267: 0.7347\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 0.5119 - auc_267: 0.8272 - val_loss: 0.6324 - val_auc_267: 0.7389\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 3s 347ms/step - loss: 0.5108 - auc_267: 0.8255 - val_loss: 0.6424 - val_auc_267: 0.7094\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 3s 334ms/step - loss: 0.4752 - auc_267: 0.8549 - val_loss: 0.6550 - val_auc_267: 0.7313\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 3s 339ms/step - loss: 0.4662 - auc_267: 0.8614 - val_loss: 0.6732 - val_auc_267: 0.7032\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 3s 335ms/step - loss: 0.4721 - auc_267: 0.8574 - val_loss: 0.6503 - val_auc_267: 0.7335\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 3s 343ms/step - loss: 0.4512 - auc_267: 0.8717 - val_loss: 0.6688 - val_auc_267: 0.7076\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 3s 309ms/step - loss: 0.4704 - auc_267: 0.8583 - val_loss: 0.6534 - val_auc_267: 0.7258\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 3s 312ms/step - loss: 0.4540 - auc_267: 0.8673 - val_loss: 0.6769 - val_auc_267: 0.7130\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 3s 319ms/step - loss: 0.4371 - auc_267: 0.8790 - val_loss: 0.6639 - val_auc_267: 0.7248\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 4s 358ms/step - loss: 0.4310 - auc_267: 0.8805 - val_loss: 0.6815 - val_auc_267: 0.7115\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 3s 336ms/step - loss: 0.4292 - auc_267: 0.8854 - val_loss: 0.6973 - val_auc_267: 0.7172\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 3s 314ms/step - loss: 0.4243 - auc_267: 0.8877 - val_loss: 0.7234 - val_auc_267: 0.7067\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=49, batch_size=128, verbose = 0)\n",
    "\n",
    "plt.plot(range(1, 50 + 1), history.history['loss'], label='Training Loss')\n",
    "plt.plot(range(1, 50 + 1), history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 36ms/step - loss: 0.6690 - auc_267: 0.6535\n",
      "Loss:  0.6689580678939819 Auc:  0.6534959077835083\n"
     ]
    }
   ],
   "source": [
    "loss, auc = model.evaluate(X_test, y_test)\n",
    "\n",
    "\n",
    "print('Loss: ', str(loss) , 'Auc: ', str(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
